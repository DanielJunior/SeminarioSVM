{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align='left' style=\"width:400px;height:120px;overflow:hidden;\">\n",
    "<a href='http://www.uff.br'>\n",
    "<img align='left' style='display: block;height: 92%' src='imgs/UFF.png' alt='UFF logo' title='UFF logo'/>\n",
    "</a>\n",
    "<a href='http://www.ic.uff.br'>\n",
    "<img align='left' style='display: block;height: 100%' src='imgs/logo-ic.png' alt='IC logo' title='IC logo'/>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Machine Learning: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Daniel Junior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Utiliza conceitos de teorias como: Teoria do Aprendizado Estatístico, Teoria das Categorias e Teoria das Restrições."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<b>Teoria do Aprendizado Estatístico</b> desenvolvida por Vapnik <i>et al.</i>, estabelece as condições matemáticas para a escolha de um classificador a partir de um conjunto de treinamento.</br>\n",
    "Definições necessárias:\n",
    "* Espaço de entradas\n",
    "* Espaço de saída\n",
    "* Considerações (Distribuição Conjunta dos dados de treinamento é uma delas.)\n",
    "* Função de Perda\n",
    "* Risco do classificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Usa um algoritmo <b>Instance Based Learning<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Isso quer dizer que um subconjunto do conjunto de treinamento será explicitamente utilizado na construção do modelo classificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Pode ser visto com uma generalização do Perceptron.\n",
    "No entanto acrescenta recursos cruciais: <b>Kernel, Maximização da Margem e Variáveis de Folga</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Possui 3 componentes principais: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Support Vectors\n",
    "* Vetor de pesos\n",
    "* Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<b>Support Vectors</b> são um subconjunto do conjunto de treinamento que será utilizado como a base de criação da <i>fronteira de decisão</i>. São a partir deles que pode ser calculada a margem máxima entre duas classes, aumentando a <i>confiança</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<b>Vetor de pesos</b> assim como nos métodos vistos anteriormente determina o grau de contribuição das features na classificação, ou ainda, qual lado do hiperplano um novo exemplo estará."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<b>Kernel</b> é uma função que tem como objetivo mapear um dado espaço em outro. Isso é utilizado para casos onde os dados não são linearmente separáveis, então é necessário definir uma transformação onde a representação dos dados neste novo espaço possa ser linearmente separável. Então se utiliza o produto escalar para definir a similaridade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM: Classificação Binária"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Margem\n",
    "* Confiança"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVM Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/svm.jpg\" style=\"margin: 0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Para o caso de dados com natureza linearmente separável, a fronteira é um hiperplano, como visto na classificação\n",
    "linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "O hiperplano que divide os dados pode ser representado por uma equação do tipo:\n",
    "$$f(x) = w \\cdot x + b = 0$$\n",
    "\n",
    "Podemos utilizar uma função para auxiliar na obtenção das classificações:\n",
    "$$g(x) = sgn(f(x)) = \\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  +1,\\quad \\textrm{se}\\quad w \\cdot x + b > 0\\\\\n",
    "                  −1,\\quad \\textrm{se}\\quad w \\cdot x + b < 0\n",
    "                \\end{array}\n",
    "              \\right.$$ \n",
    "                      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problema\n",
    "<img src=\"imgs/margin.jpg\" style=\"margin: 0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Após alguns passos resulta que o problema de maximização da margem \n",
    "de separação dos dados em relação ao hiperplano é o problema de otimização (<b>Primal</b>):\n",
    "\n",
    "$$ Minimizar_{w,b} \\quad \\frac{1}{2} \\left| \\left| w \\right| \\right|² \\quad \\quad \\textrm{(1)} $$ \n",
    "\n",
    "\n",
    "$$ \\textrm{Com as restrições:} \\quad y_i(w · x_i + b) − 1 >= 0; \\quad ∀i = 1, . . . , n \\quad \\quad \\textrm{(2)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Essa restrição diz respeito à não inclusão de pontos entre os hiperplanos auxiliares e o hiperplano separador. É uma combinação da g(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "O problema de otimização obtido é quadrático, cuja solução possui uma ampla e estabelecida teoria matemática. Como a função objetivo sendo minimizada é convexa e os pontos que satisfazem as restrições formam um conjunto convexo, esse problema possui um único mínimo global . Problemas desse tipo podem ser solucionados com a introdução de uma função Lagrangiana, que engloba as restrições à função objetivo, associadas a parâmetros denominados multiplicadores de Lagrange $\\alpha_i$.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ao introduzir a função Lagrangiana teremos a transformação para um outro problema de otimização (<b>Dual</b>), que depende apenas dos multiplicadores αi.\n",
    "$$ Maximizar_\\alpha W(\\alpha)=\\quad \\sum\\limits_{i=1}^n \\alpha_i -  \\frac{1}{2}\\sum\\limits_{i,j=1}^n \\alpha_i \\alpha_j y_i y_j<\\phi_i,\\phi_j> \\quad \\quad \\textrm{(3)} $$ \n",
    "\n",
    "\n",
    "$$ \\textrm{Com as restrições:} \\quad 0 \\leq \\alpha_i \\leq C \\quad $$\n",
    "$$ \\sum\\limits_{i=1}^n \\alpha_i y_i $$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Resolvendo o problema de otimização anterior temos o seguinte resultado:\n",
    "    $$ g(x) = sgn(f(x)) = sgn(\\sum\\limits_{x_i \\in SV} y_i α_i^∗ x_i · x + b^∗) \\quad \\quad (4)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Margens Rígidas vs Margens Suaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a href=\"#\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 0.5em;\" target=\"_blank\">\n",
    "<img src=\"imgs/rigida.png\" />\n",
    "</a><a href=\"#\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;\" target=\"_blank\"></a><a href=\"#\" target=\"_blank\"><img src=\"imgs/suave.png\" /></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<b>Margens rígidas: </b>Dado um hiperplano separador, o objetivo é maximizar a distância para os hiperplanos paralelos à ele, entre os quais não existem nenhum dado de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<b>Margens suaves:</b> Relaxa a restrição da margem rígida, permitindo algumas (limitadas) violações. Para isso na equação utilizada na maximização da margem são introduzidas <i>variáveis de folga</i> às equações (1) e (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Aqui podemos fazer a observação da percepção do SVM ser uma \"modernização\" do Perceptron. A fronteira de decisão no\n",
    "caso da classificação binária, é uma equação que retorna -1,1 como resposta de prediçao. No entanto os recursos de kernel, maximização da margem e variáveis de folga o tornam muito mais poderoso. São estes recursos que também o ajudam a evitar o overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "É possível perceber que temos um problema de otimização de difícil solução analítica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmo SMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A abordagem clássica (<i>Quadratic Programming Solvers</i>) para resolver o problema de otimização que temos são inviáveis para grande datasets, por questões de tempo e consumo de memória.\n",
    "<br/><br/>\n",
    "O <b>SMO</b>, proposto por Jonh Platt, reduz esse problema a resolver muitos sub-problemas menores de otimização envolvendo apenas dois multiplicadores de Lagrange por vez. Ou seja, utiliza uma abordagem iterativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class SVM():\n",
    "    \n",
    "    def __init__(self, max_iter=10000, kernel_type='linear', C=1.0, epsilon=0.001):\n",
    "        self.kernels = {\n",
    "            'linear' : self.kernel_linear,\n",
    "            'quadratic' : self.kernel_quadratic\n",
    "        }\n",
    "        self.max_iter = max_iter\n",
    "        self.kernel_type = kernel_type\n",
    "        self.C = C\n",
    "        self.epsilon = epsilon\n",
    "    def fit(self, X, y):\n",
    "        # Inicialização\n",
    "        n, d = X.shape[0], X.shape[1]\n",
    "        alpha = np.zeros((n))\n",
    "        kernel = self.kernels[self.kernel_type]\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            alpha_prev = np.copy(alpha)\n",
    "            for j in range(0, n):\n",
    "                i = self.get_rnd_int(0, n-1, j) # random int i~=j\n",
    "                x_i, x_j, y_i, y_j = X[i,:], X[j,:], y[i], y[j]\n",
    "                k_ij = kernel(x_i, x_i) + kernel(x_j, x_j) - 2 * kernel(x_i, x_j)\n",
    "                if k_ij == 0:\n",
    "                    continue\n",
    "                alpha_prime_j, alpha_prime_i = alpha[j], alpha[i]\n",
    "                (L, H) = self.compute_L_H(self.C, alpha_prime_j, alpha_prime_i, y_j, y_i)\n",
    "\n",
    "                # Computando os parametros do modelo\n",
    "                self.w = self.calc_w(alpha, y, X)\n",
    "                self.b = self.calc_b(X, y, self.w)\n",
    "\n",
    "                # Computando E_i, E_j\n",
    "                E_i = self.E(x_i, y_i, self.w, self.b)\n",
    "                E_j = self.E(x_j, y_j, self.w, self.b)\n",
    "\n",
    "                # Novos valores de alpha\n",
    "                alpha[j] = alpha_prime_j + float(y_j * (E_i - E_j))/k_ij\n",
    "                alpha[j] = max(alpha[j], L)\n",
    "                alpha[j] = min(alpha[j], H)\n",
    "\n",
    "                alpha[i] = alpha_prime_i + y_i*y_j * (alpha_prime_j - alpha[j])\n",
    "\n",
    "            # Convergência\n",
    "            diff = np.linalg.norm(alpha - alpha_prev)\n",
    "            if diff < self.epsilon:\n",
    "                break\n",
    "\n",
    "            if count >= self.max_iter:\n",
    "                print(\"As iterações excederam o limite de %d iterações.\" % (self.max_iter))\n",
    "                return\n",
    "        # Computando os parametros finais do modelo\n",
    "        self.b = self.calc_b(X, y, self.w)\n",
    "        if self.kernel_type == 'linear':\n",
    "            self.w = self.calc_w(alpha, y, X)\n",
    "        # Recuperando os vetores de suporte\n",
    "        alpha_idx = np.where(alpha > 0)[0]\n",
    "        support_vectors = X[alpha_idx, :]\n",
    "        return support_vectors, count\n",
    "    def predict(self, X):\n",
    "        return self.h(X, self.w, self.b)\n",
    "    def calc_b(self, X, y, w):\n",
    "        b_tmp = y - np.dot(w.T, X.T)\n",
    "        return np.mean(b_tmp)\n",
    "    def calc_w(self, alpha, y, X):\n",
    "        return np.dot(alpha * y, X)\n",
    "    # Predição\n",
    "    def h(self, X, w, b):\n",
    "        return np.sign(np.dot(w.T, X.T) + b).astype(int)\n",
    "    # Erro de predição\n",
    "    def E(self, x_k, y_k, w, b):\n",
    "        return self.h(x_k, w, b) - y_k\n",
    "    def compute_L_H(self, C, alpha_prime_j, alpha_prime_i, y_j, y_i):\n",
    "        if(y_i != y_j):\n",
    "            return (max(0, alpha_prime_j - alpha_prime_i), min(C, C - alpha_prime_i + alpha_prime_j))\n",
    "        else:\n",
    "            return (max(0, alpha_prime_i + alpha_prime_j - C), min(C, alpha_prime_i + alpha_prime_j))\n",
    "    def get_rnd_int(self, a,b,z):\n",
    "        i = z\n",
    "        while i == z:\n",
    "            i = rnd.randint(a,b)\n",
    "        return i\n",
    "    # Kernels\n",
    "    def kernel_linear(self, x1, x2):\n",
    "        return np.dot(x1, x2.T)\n",
    "    def kernel_quadratic(self, x1, x2):\n",
    "        return (np.dot(x1, x2.T) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def calc_acc(y, y_hat):\n",
    "    idx = np.where(y_hat == 1)\n",
    "    TP = np.sum(y_hat[idx] == y[idx])\n",
    "    idx = np.where(y_hat == -1)\n",
    "    TN = np.sum(y_hat[idx] == y[idx])\n",
    "    return float(TP + TN)/len(y)\n",
    "\n",
    "\n",
    "def readData(filename, header=True):\n",
    "    data, header = [], None\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        if header:\n",
    "            header = spamreader.next()\n",
    "        for row in spamreader:\n",
    "            data.append(row)\n",
    "    return (np.array(data), np.array(header))\n",
    "\n",
    "filename = \"data/iris-virginica.txt\"\n",
    "(data, _) = readData(filename, header=False)\n",
    "data = data.astype(float)\n",
    "\n",
    "positives = np.array([np.array(row) for row in data[:] if row[4] == 1])\n",
    "negatives = [np.asarray(row) for row in data[:] if row[4] == -1]\n",
    "\n",
    "training_data = positives[:int(len(positives) * 0.8)]\n",
    "neg = negatives[:int(len(negatives) * 0.8) ]\n",
    "training_data = np.concatenate((training_data,neg))\n",
    "\n",
    "test_data = positives[int(len(positives)* 0.8 ) :]\n",
    "neg = negatives[int(len(negatives) * 0.8) :]\n",
    "test_data = np.concatenate((test_data,neg))\n",
    "\n",
    "X  = training_data[:,0:-1]\n",
    "y = training_data[:,-1].astype(int)\n",
    "\n",
    "X_t = test_data[:,0:-1]\n",
    "y_t = test_data[:,-1].astype(int)\n",
    "\n",
    "model = SVM()\n",
    "\n",
    "# Treinamento\n",
    "support_vectors, iterations = model.fit(X, y)\n",
    "\n",
    "# Quantidade de vetores de suporte\n",
    "sv_count = support_vectors.shape[0]\n",
    "\n",
    "y_hat_training = model.predict(X)\n",
    "acc_training = calc_acc(y, y_hat_training)\n",
    "\n",
    "y_hat_test = model.predict(X_t)\n",
    "acc_test = calc_acc(y_t, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de vetores de suporte: 61\n",
      "Viés:\t\t168.213\n",
      "Pesos:\t\t[ -8.08240247  -3.25488021 -26.69166755 -14.23671075]\n",
      "Convergência em 8 iterações.\n",
      "\n",
      "********** Dados de treinamento ************\n",
      "\n",
      "Acurácia:\t0.775\n",
      "\n",
      "********* Dados de teste *************\n",
      "\n",
      "Acurácia:\t0.433\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantidade de vetores de suporte: %d\" % (sv_count))\n",
    "print(\"Viés:\\t\\t%.3f\" % (model.b))\n",
    "print(\"Pesos:\\t\\t\" + str(model.w))\n",
    "print(\"Convergência em %d iterações.\" % (iterations))\n",
    "\n",
    "print(\"\\n********** Dados de treinamento ************\\n\")\n",
    "print(\"Acurácia:\\t%.3f\" % (acc_training))\n",
    "\n",
    "print(\"\\n********* Dados de teste *************\\n\")\n",
    "print(\"Acurácia:\\t%.3f\" % (acc_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "No entanto, a maior parte dos problemas reais não possui todas as características que são necessárias para aplicação da solução acima. Mais especificamente, os dados não podem ser separados por um hiperplano no espaço original das entradas. Então é necessário realizar o mapeamento utilizando alguma função Kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVM Não-Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/nonlinear.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em termos práticos a transformação de um conjunto de entrada de um espaço para outro demanda um esforço computacional muito grande.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A solução é utilizar uma função de similaridade que possa ser calculada em termos do espaço original, mas com seu resultado referindo ao espaço transformado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Utilizando uma função de kernel $\\theta$ apropriada, alteramos a equação (3) e obtemos o seguinte classificador:\n",
    "    $$ g(x) = sgn(f(x)) = sgn(\\sum\\limits_{x_i \\in SV} y_i α_i^∗ \\phi (x_i) · \\phi (x) + b^∗) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model_2 = SVM(kernel_type='quadratic')\n",
    "\n",
    "# Fit model\n",
    "support_vectors_2, iterations_2 = model_2.fit(X, y)\n",
    "\n",
    "# Support vector count\n",
    "sv_count_2 = support_vectors_2.shape[0]\n",
    "\n",
    "y_hat_training_2 = model_2.predict(X)\n",
    "acc_training_2 = calc_acc(y, y_hat_training_2)\n",
    "\n",
    "y_hat_test_2 = model_2.predict(X_t)\n",
    "acc_test_2 = calc_acc(y_t, y_hat_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de vetores de suporte: 68\n",
      "Viés:\t\t0.772\n",
      "Pesos:\t\t[ 0.04925745  0.0445175  -0.20086825 -0.13343222]\n",
      "Convergência em 10 iterações.\n",
      "\n",
      "********** Dados de treinamento ************\n",
      "\n",
      "Acurácia:\t1.000\n",
      "\n",
      "********* Dados de teste *************\n",
      "\n",
      "Acurácia:\t0.967\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantidade de vetores de suporte: %d\" % (sv_count_2))\n",
    "print(\"Viés:\\t\\t%.3f\" % (model_2.b))\n",
    "print(\"Pesos:\\t\\t\" + str(model_2.w))\n",
    "print(\"Convergência em %d iterações.\" % (iterations_2))\n",
    "\n",
    "print(\"\\n********** Dados de treinamento ************\\n\")\n",
    "print(\"Acurácia:\\t%.3f\" % (acc_training_2))\n",
    "\n",
    "print(\"\\n********* Dados de teste *************\\n\")\n",
    "print(\"Acurácia:\\t%.3f\" % (acc_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Referências\n",
    "\n",
    "* <a href=\"http://seer.ufrgs.br/rita/article/view/rita_v14_n2_p43-67\">Uma Introdução às Support Vector Machines</a>\n",
    "* <a href=\"https://www.cs.cmu.edu/~ggordon/SVMs/new-svms-and-kernels.pdf\">Support Vector Machines and Kernel Methods</a>\n",
    "* <a href=\"http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html\">Everything You Wanted to Know about the Kernel Trick (But Were Too Afraid to Ask)</a>\n",
    "* <a href=\"http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf\">An Idiot’s guide to Support vector machines (SVMs)</a>\n",
    "* <a href=\"http://research.microsoft.com/pubs/68391/smo-book.pdf\"> Fast Training of Support Vector Machines using Sequential Minimal Optimization</a>\n",
    "* <a href=\"http://nshorter.com/ResearchPapers/MachineLearning/A_Roadmap_to_SVM_SMO.pdf\">A Roadmap to SVM Sequential Minimal\n",
    "Optimization for Classification </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
